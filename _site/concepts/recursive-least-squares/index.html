<!DOCTYPE html>
<html lang="en-us">

  <head>
  <link href="http://gmpg.org/xfn/11" rel="profile">
  <meta http-equiv="content-type" content="text/html; charset=utf-8">

  <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1">

  <title>
    
      Recursive Least Squares Estimation (LSE) &middot; Akhil Sankar
    
  </title>

  
  <link rel="canonical" href="http://localhost:4000/concepts/recursive-least-squares/">
  

  <link rel="stylesheet" href="http://localhost:4000/public/css/poole.css">
  <link rel="stylesheet" href="http://localhost:4000/public/css/syntax.css">
  <link rel="stylesheet" href="http://localhost:4000/public/css/lanyon.css">
  <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=PT+Serif:400,400italic,700%7CPT+Sans:400">

  <link rel="apple-touch-icon-precomposed" sizes="144x144" href="http://localhost:4000/public/apple-touch-icon-precomposed.png">
  <link rel="shortcut icon" href="http://localhost:4000/public/favicon.ico">

  <link rel="alternate" type="application/rss+xml" title="RSS" href="http://localhost:4000/atom.xml">

  
  <script>
    (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
    (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
    m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
    })(window,document,'script','//www.google-analytics.com/analytics.js','ga');
    ga('create', 'UA-XXXX-Y', 'auto');
    ga('send', 'pageview');
  </script>
  <script type="text/javascript" async
    src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/3.2.2/es5/tex-mml-chtml.js">
  </script>
  
</head>


  <body>

    <!-- Target for toggling the sidebar `.sidebar-checkbox` is for regular
     styles, `#sidebar-checkbox` for behavior. -->
<input type="checkbox" class="sidebar-checkbox" id="sidebar-checkbox">

<!-- Toggleable sidebar -->
<div class="sidebar" id="sidebar">
  <div class="sidebar-item">
    <p>A reserved <a href="https://jekyllrb.com" target="_blank">Jekyll</a> theme that places the utmost gravity on content with a hidden drawer. Made by <a href="https://twitter.com/mdo" target="_blank">@mdo</a>.</p>
  </div>

  <nav class="sidebar-nav">
    <a class="sidebar-nav-item" href="http://localhost:4000/">About Me</a>
    <a class="sidebar-nav-item" href="/projects">Projects</a>
    <a class="sidebar-nav-item" href="/concepts">Concepts</a>

    <!-- <a class="sidebar-nav-item" href="/archive/v1.1.0.zip">Download</a> -->
    <span class="sidebar-nav-item">Currently v1.1.0</span>
  </nav>

  <div class="sidebar-item">
    <p>
      &copy; 2025. All rights reserved.
    </p>
  </div>
</div>


    <!-- Wrap is the content to shift when toggling the sidebar. We wrap the
         content to avoid any CSS collisions with our real content. -->
    <div class="wrap">
      <div class="masthead">
        <div class="container">
          <h3 class="masthead-title">
            <a href="/" title="Home">Akhil Sankar</a>
            <small></small>
          </h3>
        </div>
      </div>

      <div class="container content">
        <div class="page">
  <h1 class="page-title">Recursive Least Squares Estimation (LSE)</h1>
  <h3 id="high-level-strategy">High Level Strategy</h3>

<p>By definition, <a href="/concepts/least-squares/">LSE</a> and <a href="/concepts/weighted-least-squares">weighted LSE</a> approaches are retroactive in nature. The best-fit model is only derived after the entire dataset is compiled and available. Both techniques require storing and updating the \(H\) and \(\bar{Y}\) components (encoding the entire dataset and adding new rows thereafter) as new data comes in. This can be too slow (not real-time enough depending on frequency of incoming datapoints) and computationally expensive.</p>

<p>Instead, we want to estimate the best-fit model in real-time (as a new datapoint/measurement comes in). Specifically:</p>
<ul>
  <li>do not store all the measurements upto the current timestep</li>
  <li>simply store the previous best estimate and update it with new data</li>
</ul>

<p>Thus, we want the workflow of the recursive LSE to look like this:</p>

<p><img src="/assets/images/recursive-least-squares/recursive_lse_cycle.png" alt="Range of Uncertainty" /></p>

<p>In this diagram, \(\hat{X}_{k-1}\) represents the best-fit model parameters based on the dataset upto the timestep \(k-1\). Once a new datapoint \(y_k\) is introduced, the new model parameters \(\hat{X_{k}}\) are calculated based purely on the previously calculated parameters and the new datapoint.</p>

<h3 id="recursively-updating-the-model-parameters-hatx_k">Recursively Updating the Model Parameters \(\hat{X}_{k}\)</h3>

<p>We can start by reordering <a href="/concepts/least-squares/">LSE \((2)\)</a>:</p>

\[% \quad \bar{R} = H \bar{X} - \bar{Y} \quad
\quad \bar{Y} = H \bar{X} + \bar{R} \quad \tag{1}\]

<p>Since we are considering the general case of LSE where datapoints are stochastic (\(\bar{R}\) can also be thought of as random noise), let’s also recall the properties of our (stochastic) datapoints from <a href="/concepts/least-squares/">the Weighted LSE Problem</a> using the Expectation Operator \(E()\) such that:</p>

<ul>
  <li>\(E(\bar{R}) = 0\) : unbiased, white noise
    <ul>
      <li>mean of the noise (or residual error) across all datapoints is 0</li>
    </ul>
  </li>
  <li>\(E({r_i}{r_j}^T) = 0\) : no correlation between datapoints
    <ul>
      <li>mean of the covariance between any two datapoints is 0</li>
      <li>this stored in the covariance matrix \(C\)</li>
    </ul>
  </li>
  <li>\(E({r_i}{r_i}^T) = C_i\) : this is the variance of each measurement. The covariance matrix \(C\) stores these along its diagonal.</li>
</ul>

<p><span style="color:red">Big Picture: The general strategy is to update the model parameters based on the difference between the predicted value (model output) and the actual datapoint. We call this the error or <em>residual</em> \(r_k\) (as we have seen in all of the LSE approaches thus far). We want to therefore update the model parameters by a proportional value \(K_k\) to the magnitude of the residual value. We call \(K_k\) the <em>correction gain</em>. </span></p>

<p>Thus, the recursive update looks like this:</p>

\[\hat{X}_k = \hat{X}_{k-1} + K_k(R_k)\]

<p>where \(R_k = H\hat{X}_{k-1} - Y_k\) :</p>

\[\Rightarrow \boxed{\hat{X}_k = \hat{X}_{k-1} + K_k(H\hat{X}_{k-1} - Y_k)}\]

<p>Next, we want to set up this update as an optimization problem in order to minimize the error between our estimate of the best-fit model \(\hat{X}_k\) and the notion of a theoretical best-fit model \(X\).</p>

<p>We will call this error \(\epsilon_k\) such that:</p>

\[\epsilon_k = X - \hat{X}_k\]

\[\Rightarrow \epsilon_k = X - (\hat{X}_{k-1} + K_k(H\hat{X}_{k-1} - Y_k))\]

<p>We can also note that \(\epsilon_{k-1} = X - \hat{X}_{k-1}\) so:</p>

\[\epsilon_k = \epsilon_{k-1} - K_k(Y_k - H\hat{X}_{k-1})\]

<p>Next, we can substitute for \(Y_k\) using \((1)\). In keeping with our current notation, we will re-write \(\bar{X}\) as the more general notation \(X\) to represent the theoretical optimal best-fit parameters. These are the same thing:</p>

\[\epsilon_k = \epsilon_{k-1} - K_k\big[(HX + R_{k}) - H\hat{X}_{k-1}\big]\]

\[\Rightarrow \epsilon_{k-1} - K_k\big[(H(X - \hat{X}_{k-1}) + R_{k}\big]\]

\[\Rightarrow \epsilon_{k-1} - K_k\big[H\epsilon_{k-1} + R_{k}\big]\]

\[\Rightarrow \epsilon_{k-1} - K_{k}H\epsilon_{k-1} - K_{k}R_{k}\]

<p>Thus:</p>

\[\epsilon_k = \big(I - K_{k}H\big)\epsilon_{k-1} - K_{k}R_{k}\]

<p>If we consider this expression more closely, we can see that:</p>
<ul>
  <li>the estimation error \(\epsilon_k\) is a function of the previous estimation error and current noise.</li>
  <li><span style="color: red">(#TODO: does this make sense?)</span>: as long as \(K_{k}H &lt; I\), the new estimation error will be smaller than the previous estimation error.</li>
</ul>

<p>Recall that we want to minimize the error between the theoretically best-fit parameters \(X\) and the estimated best-fit parameters \(\hat{X}_k\). Since we are recursively updating our estimated best-fit parameters, each update will have some error associated with it.</p>

<p>For further intuition, consider this pattern:</p>

<ul>
  <li>timestep 1: start with i datapoints and calculate the best-fit line parameters \(\hat{X}_1\). These estimated best-fit parameters might have some estimation error against the overall theoretical best-fit line parameters \(X\) called \(\epsilon_1 = X - \hat{X}_1\)</li>
  <li>timestep 2: when a new datapoint is added, perform the same action. This yields a new estimation error is called \(\epsilon_{2}\)</li>
  <li>… and so on</li>
</ul>

<p>Similar to our approach with the residuals, we want to square these estimation error components. See the <a href="/concepts/least-squares/">explanation in general LSE</a> for why this is done.</p>

<p>Thus the overall error of the recursive LSE function is the sum of each of these estimation errors. We can call this the overall cost of our error function \(J_k\) such that:</p>

\[J_k = \epsilon_1^{2} + \epsilon_2^{2} + ... + \epsilon_k^{2}\]

\[\Rightarrow J_k =  (X - \hat{X}_1)^2 + (X - \hat{X}_2)^2 + ... + (X - \hat{X}_k)^2\]

<p>If we represent each of our \(\epsilon_i\) components in one vector called \(\epsilon\), we can rewrite our cost function more compactly as:</p>

\[\Rightarrow J_k =  \epsilon^{T}\epsilon\]

<p>Thus, we can observe that \(J_k\) is expressed through a quadratic function (convex parabola as proven in <a href="/concepts/least-squares/">LSE</a>. This cost is the net error of the recursive LSE function.</p>

<p>Since our control input to drive the estimation error down is \(K_k\) (the gain matrix), we can take the derivative of \(J_k\) with respect to \(K_k\):</p>

\[\frac{\partial J_k}{\partial K_k} = 0 \Rightarrow 
-2 (I - K_k H) P_{k-1} H^T + 2 K_k C_k\]

<p>where \(P_{k}\) can be expressed as:</p>

\[P_k = \epsilon_k \epsilon_k^T\]

\[\Rightarrow \big[(I - K_k H_k) \epsilon_{k-1} - K_k R_k\big]
\big[ (I - K_k H_k) \epsilon_{k-1} - K_k r_k \big]^T\]

\[= (I - K_k H_k) P_{k-1} (I - K_k H_k)^T + K_k R_k K_k^T\]

<p>Thus, it works out that our gain matrix \(K_k\) can be expressed as:</p>

\[\boxed{K_k = P_{k-1} H_k^T \left( H_k P_{k-1} H_k^T + C_k \right)^{-1}}\]

<!-- Now that we have an expression for $$\epsilon_k$$,  -->


</div>

      </div>
    </div>

    <label for="sidebar-checkbox" class="sidebar-toggle"></label>

    <script src='/public/js/script.js'></script>
  </body>
</html>
